\documentclass[10pt]{IEEEtran}
\usepackage[T1]{fontenc}
%\documentclass[12pt]{Article}
%\renewcommand\rmdefault{phv}
%\usepackage[doublespacing]{setspace}
%\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[fleqn]{amsmath}
\usepackage{blindtext}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{indentfirst}
\usepackage{authblk}
\PassOptionsToPackage{bookmarks=false}{hyperref}
\hypersetup{draft}
\usepackage{todonotes}
\usepackage{svg}
\usepackage{textcomp}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\newcommand*\Append[2]{\State #1$.append($#2$)$}
\providecommand{\myceil}[1]{\left \lceil #1 \right \rceil }
\tikzset{declare function={Ceil(\x)=round(\x+0.5);}}
\usepackage[backend=biber]{biblatex}
\addbibresource{ZTree.bib}

\title{Enabling Sybil Resistant P2P Range Queries Through Application of Blockchain-Based Consensus}
\author{Charles Noyes \\ cnoyes@usc.edu}
\date{\vspace{-5ex}}

\begin{document}

\maketitle

\begin{abstract}
We present the design of a novel data structure. We aim to solve the issue of window/range-based queries of distributed data stores in peer to peer architectures. Traditional models, for example, distributed hash tables (DHTs), are hostile towards window queries because their hashing operations are designed to uniformly distribute stored data across a defined key space; the hashing operations used to achieve this even key distribution inherently erase all characteristics of the target data that could be used to classify it. We solve this problem of erasure by defining a scheme in which higher-order data is mapped to a first-dimensional key space, while preserving locality. The resulting key space is not uniformly distributed, so we define a distributed consensus protocol in which participants in our network agree to target highly populated regions of the key space, through deterministic redistribution. This consensus scheme also provides some protection from Sybil attacks, as it is a to hash-based proof-of-work system. Finally, we define storage, lookup, and deletion operations that efficiently perform necessary network functions. A peer to peer communication system acts as the underlying network for participants, providing all of the traditional benefits of a P2P architecture.
\end{abstract}


\section{Notation}

\noindent Tuples/Lists: $[a,b,c,...]$

\noindent Application of a function $f$ to an input $x$: $y=f(x)$

\noindent Application of a hash function $H$ with $k$ output bits: $H_{k}(x) = \{0,1\}^{[k]}$

\noindent Keys of user $j$: $ PUBK_j; PRIVK_j $

 
\section{Introduction}
\par With the growing prevalence of decentralized networks, a corollary need for data stores that are able to operate in these networks exists. Distributed hash tables are, arguably, the most widespread distributed data structure for these purposes~\cite{Stoica:2001dj,Rowstron:2001ea,Ratnasamy:2001wn}. They are used in many large scale open sources projects~\cite{Freitas:2013tb,Xu:2010vs,Perfitt:2010fh} as their underlying infrastructure. First, in order to better understand the motivation behind this project, a brief explanation of the workings of a distributed has table (DHT) and its constituent protocols is needed.

\par Simply put, DHTs were created to provide hash table-like functionality on an Internet-scale level~\cite{Ratnasamy:2001wn}. They expose basic PUT and GET requests (operating on (key,value) pairs) to clients, allowing them to store and retrieve information from the DHT. The difference between a hash table and a DHT is the scope of the mechanism: DHTs are able to operate in an entirely decentralized setting, in which the responsibility for the mapping of keys to values is distributed pseudo-uniformly among all participants in a P2P network. Notable networks that utilize DHTs include the Coral Content Distribution Network~\cite{Freedman:2004vb}, the Storm Botnet~\cite{Holz:2008uk}, and the BitTorrent file sharing protocol~\cite{Cohen:y1_8mBnw}.

\par Keys used in traditional DHTs are calculated through the use of a hash function. Given that the digest (output) of a hash function is some number of bits, each of which has the same probability of being set to $'1'$ or $'0'$, we can confidently conclude that the outputs will be randomly distributed along the hash function's key space~\cite{Stoica:2001dj}. This keyspace is the set of all possible outputs; for a hash function with $k$ output bits, this is the set $\{0,1\}^k$. This is useful to users in a DHT because, assuming that a given hash function is 'sound,' in that its outputs are actually evenly distributed, they are able to mathematically verify the even partitioning of the key space. The participants can thus trust that the load will be balanced fairly among all nodes. No one node will have complete control, and no group of nodes will be unfairly burdened due to their residing in some supposed 'hotspots' in the key space, in which a disproportionally high number of objects are stored.

\par One important continuity between all of these examples is the use of a DHT in an 'exact-match' lookup scenario; the even distribution of hash functions once again comes into play. The purging of any locality (that is, the property of alike object's digests being somehow as 'distant' from one another as the objects themselves) precludes the possibility of range or window-based queries. Ramabhadran et al. put it very well: \textit{"However, range queries, asking for all objects with values in a certain range, are particularly difficult to implement in DHTs. This is because DHTs use hashing to distribute keys uniformly and so can't rely on any structural properties of the key space, such as an ordering among keys"}~\cite{Ramabhadran:2004tr}\textit{.}

The idea of locality vs. evenness is fundamental to our work, and so we define it here. Intuitively, we can say that locality is preserved for any given set of objects (serializable to a numeric representation in some fashion) $A,B,$ and $C$ in a function $f$ such that
\begin{equation} \label{eq:locality}
|A-B| < |B-C| \Rightarrow |f(A)-f(B)| < |f(B) - f(C)|,
\end{equation}
and garden variety hash functions obviously do not fulfill this requirement.

\section{Related Work}
\par Some research has been done on the usage of a DHT as an overlay on top of a traditional data structure to enable range queries~\cite{Ramabhadran:2004tr,Desnoyers:2008uo}~\todo{add self-chord citation}, however none of the currently proposed solutions are useful in a true P2P network. Sybil attacks are arguably the most severe vulnerability in decentralized networks~\cite{Douceur:2002jr}, with a litany of papers having been published on this attack in the context of DHTs (e.g.~\cite{LesniewskiLass:2010ue}), and yet the subject is not even broached. To summarize, Sybil attacks are the orchestrated undermining or subversion of reputation or voting systems through the bulk forging of a identities in P2P networks.



\section{Our Approach}
\begin{figure}[!t]
\centering
\includegraphics[width=3in]{ZCurve}
\caption{Morton Codes/Z-Order Curving~\cite{ZorderCurve:2015vo}}
\label{fig_MCode}
\end{figure}

\par The Squid project~\cite{Schmidt:2003cd}, another tangentially related work, has actually tried to define protocols for more generalized searching (e.g. range and keyword queries), in much the same way we have. The similarities end in the applicability of their work to situations that are not just distributed in a peer to peer network, but truly decentralized. The differences between the two are slight but of paramount importance to the applicability of a protocol that operates correctly only in the first case; it is relatively easy to distribute a process across a trusted network, but decentralization is another beast entirely.

\par A DHT that does work in a decentralized fashion would be enormously useful in a variety of settings. No data structure in existence has these capabilities and this work is, in fact, the direct result of the inability to find an existing solution. Some potential applications include scalable content addressable networks, decentralized applications, and sensor/information networks that potentially operate in the presence of adversaries.




\par We believe that the best solution is one in which we only slightly modify the way in which a DHT works. Substituting a space filling curve for a hash function would allow us to preserve locality of data in a DHT with rigorously defined limitations on what kind of data may be stored in it. A space filling curve brings its own set of limitations, however, and so we also present a method of solving these problems.

\subsection{Hash Function Replacement}

\par Currently, we believe that bit interleaving is a promising replacement for hash functions. Through the use of space filling curves, such as a Z-Order curve (sometimes called the Morton curve), $Nth$-dimensional data can be mapped to the first dimension (such that it may be stored in-memory). The mapping of a section of the integral Cartesian plane to binary, through the interleaving of the $x$ and $y$ coordinates, is illustrated in Fig.~\ref{fig_MCode}.

\par Interestingly, the interleaving of bits to form Morton codes preserves the locality of the initial data in the output, satisfying Eq.~\ref{eq:locality}. There is, however, no currently agreed upon scheme for the extension of this function to floating point numbers. A few solutions have been proposed, such as~\cite{Connor:2010eq}, but none have been truly validated. We do not believe this to be a significant obstacle, and even an imperfect solution can work for the time being.

\par For now, we are using longitudinal and latitudinal coordinates to test range based searches in a sorted key-value database. Our algorithm to serialize the decimal coordinates to bytes is as follows, where $lat$ and $lon$ are the inputs and $ZOrd$ is a bit interleaving function:

\begin{equation} \label{eq:serialization}
ZOrd(int((lat + 90) * 100), int((lon + 180) * 100))
\end{equation}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{errors.eps}
\caption{Bit Interleaving With A Z-Order Curve}
\label{fig_ZOrd}
\end{figure}

\par Obviously this is an imperfect solution, however the number of errors in a real-world test of this algorithm showed that the error rate is fairly negligible. To measure lookup accuracy on a large corpus, we performed a range query (note: the range was always the same) on data sets of varying sizes, using \ref{eq:serialization} as our serialization scheme.
% \begin{algorithm}
%   \begin{algorithmic}[1]
%   \Require{A '/' denotes two variables with different prefixes, and is used to save line space.}
%   \vspace{-0.4cm}
%     \Statex
%     \Function{Test}{$n,min/maxLat,min/maxLon$}
%         \Let{$storageDic$}{$\{\}$}
%         \Let{$correctResults$}{$[]$}
%         \Let{$realResults$}{$[]$}
%         \For{$i \textrm{ in Range}(n)$}
%             \Let{$lat$}{$\textrm{RandFloat}(-90,90)$}
%             \Let{$lon$}{$\textrm{RandFloat}(-180,180)$}
%             \Let{$key$}{$Eq.$~\ref{eq:serialization}$(lat,lon)$}
%             \Let{$storageDic[key]$}{$i$}
%             \If{$(minLat<lat<maxLat)\&\&$ \par 
%             \hskip\algorithmicindent$(minLon<lon<maxLon)$}
%                 \Append{$correctResults$}{$i$}
%             \EndIf
%         \EndFor
%         \Let{$minCode$}{$Eq.~\ref{eq:serialization}(minLat/Lon)$}
%         \Let{$maxCode$}{$Eq.~\ref{eq:serialization}(maxLat/Lon)$}
%         \For{$key \textrm{ in } storageDic.keys()$}
%             \If{$minCode < key < maxCode$}
%                 \Append{$realResults$}{$key$}
%             \EndIf
%         \EndFor
%         \State \Return{$len(correctResults - realResults)$}
        
%     \EndFunction
%   \end{algorithmic}
% \end{algorithm}

\par Our results are shown in Fig.~\ref{fig_ZOrd}. We are confident that this decimal flooring scheme will effectively bridge the gap of time between the work we are doing here and the validation of a floating point-ready Z-Ordering scheme.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{finalRing}
\caption{Evenly Distributed Key Space Mapping}
\label{fig_kSpace}
\end{figure}

\subsection{Node Rearrangement}
\par Next, we come to the problem of key space distribution. Illustrated in Fig.~\ref{fig_kSpace} is the way in which we transform the linear version of the key space into a circular representation, and how the nodes communicate with their 'neighbors' in a DHT. On one 'end' of the linear representation is the lowest possible value (in binary, this would be the set $\{0\}^{[k]}$), and on the other is the maximum possible value ($\{1\}^{[k]}$). Notice that in both representations, the nodes are distributed evenly. Each occupies the same amount of space as the others, and together they cover the entire key space. We don't even need to populate the key space with examples, as we can prove that they will cover it evenly when using a 'good' hash function as the key function.

\par If we were to use a Z-Ordering scheme as the key space location function, we will almost certainly have the data become 'clustered' in some sense. For example, if we were to store all of the data about the global land-line network, we would most likely have all of the data from highly populated areas focused on a few regions in the key space, and oceans of nothingness between them. This is an unavoidable byproduct of the use of a locality-preserving function.



\par Fig.~\ref{fig_kSpaceUneven} shows a distributed data structure in which a locality preserving function was used, with red highlights gauging the fullness of each region in the key space. Notice that two of the nodes are doing nearly all of the work, while the others have no load to bear. There is no existing mechanism for nodes to retroactively redistribute themselves.


\par We propose the inclusion of cryptographic proof-of-work, ala Hashcash~\cite{Back:2002vq}, with each stored piece of data. Because these proofs are difficult to compute, no entity can easily execute a Sybil-like attack against the rest of the network by including 'junk' proofs; all others would simply be able to see that the proofs are incorrect. A user $j$ would send this to the rest of the network:

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{unevenDistro}
\caption{Node Convergence}
\label{fig_kSpaceUneven}
\end{figure}

\begin{equation} \label{eq:proof}
SerializedData = [PrivK_{j}((H(Data)), Data, PubK_{j}]
\end{equation}

\par When the nodes in a network wish to reconfigure themselves, all nodes are instructed to report all of the proofs-of-work they know about. All of the nodes then independently construct a curve of the key space density, such that they have some idea of 'saturation' in a given region, and attempt to converge onto more dense regions and away from sparsely populated regions.

\par From a game theory perspective, the proof-of-work system is greatly beneficial. Sybil attacks are largely mitigated, and there is no obvious advantage for a malicious node to withhold its knowledge of the proofs in its region. It does, however, introduce some amount of overheard into the network. We believe that this is not a terribly inhibiting factor, as the use cases for a decentralized (i.e. public) version of our data structure are almost always either altruistic or capitalistic in nature. To give an example of the former, folding@Home~\cite{Anderson:2002vr}, a network in which people donate unused computational resources to fold proteins, has been wildly successful. Bitcoin~\cite{Nakamoto:2008ti}, the decentralized P2P cryptocurrency network, is an excellent example of hashing power being used for the latter case. In both networks, there is no lack of available resources.

\par The actual methodology of the 'reporting' and 'rearrangement' is more of an implementation detail than a rigorous protocol specification. This is mostly a result of the extreme variance between individual use cases; rarely will a generalized protocol work for all cases. In section~\todo{section goes here}, some possible approaches are discussed and implemented. 

\section{Analysis}
\subsection{Mitigation of Sybil Attacks on The DHT Proper}
\par Some of the claims made thus far are provable only in the abstract. For example, we assert that Sybil attacks are mitigated through a proof-of-work system and that this work is not a growth inhibiting factor. Our proof-of-work system is a basic one, modeled on the HashCash~\cite{Back:2002vq} system. We define a 'proof' as being the value resulting of the inputting of both information and a random number to a hash function. If this proof meets the threshold of difficulty set by the network, it is accepted as valid. To show this, we assume that for a hash function of k bits, the set of possible results is exactly $\{0,1\}^{[k]}$. The odds of any $k'th$ bit being $0$ or $1$ are exactly even (in this case, $1:1$). Thus, the odds of the first $n$ bits being set to 0 is exactly $\frac{1}{2^n}$. Note that simply using $0$-prefixing as the method of work determination is simplistic, and 'better' schemes (like that of Bitcoin, which uses full hex values) can provide finer control. 

\par When applied to our data structure, we would be using the number of votes as our difficulty determining factor. The difficulty should be proportionate to the number of votes and dependant on some predefined ratio of necessary goodness (generally this will be $ > 50\%$, as a result of the Byzantine generals problem's limits in the general case). Mathematically,
\begin{multline}
N = \textrm{population size} \\
m = \textrm{proportion of wanted goodness} \\
n = \textrm{maximum difficulty} \\
q = \textrm{average number of tries} \\
z = \textrm{number of required 0 bits} \\
z = \myceil{log_2{ (\frac{2^n}{\frac{N}{m}})}}, q = 2^z \\
\end{multline}

\begin{figure}
\begin{tikzpicture}
\begin{semilogxaxis}[
    jump mark mid,
    domain=10:10000,
    xmin = 10,
    xmax = 10000,
    ymin = 0,
    ymax = (0.4 * 10^9),
    title = {Number of Calculations vs. Network Size},
    xlabel = {Number of Nodes ($N$)},
    ylabel = {Average Number of Tries ($q$)},
    title style={yshift=1.5ex}
    ]
\addplot[color=orange]{2^(Ceil(ln((4294967296) / (x / 0.8))/ln(2)))};
\addplot[color=blue]{2^(Ceil(ln((4294967296) / (x / 0.7))/ln(2)))};
\addplot[color=green]{2^(Ceil(ln((4294967296) / (x / 0.6))/ln(2)))};
\addplot[color=red]{2^(Ceil(ln((4294967296) / (x / 0.5))/ln(2)))};
\addplot[color=purple]{2^(Ceil(ln((4294967296) / (x / 0.4))/ln(2)))};
\legend{m = 0.8,m = 0.7, m = 0.6,m = 0.5,m = 0.4}
\end{semilogxaxis}
\begin{semilogxaxis}[
    domain=10:10000,
    xmin = 10,
    xmax = 10000,
    ymin = 0,
    ymax = (0.4 * 10^9),
    yticklabels={,,},
    xticklabels={,,},
    hide axis
    ]
\addplot[color=orange]{(4294967296) / (x / 0.8)};
\addplot[color=blue]{(4294967296) / (x / 0.7)};
\addplot[color=green]{(4294967296) / (x / 0.6)};
\addplot[color=red]{(4294967296) / (x / 0.5)};
\addplot[color=purple]{(4294967296) / (x / 0.4)};
\end{semilogxaxis}

\end{tikzpicture}
\caption{Scaling Validation}
\label{fig_tryGraph}
\end{figure}

\par A reference graph for $n = 32$ is shown in Fig.~\ref{fig_tryGraph}, highlighting the relation between goodness thresholds ($m$) and the average number of hash calculations needed to find a good nonce (a random number) ($q$), along varying network sizes ($N$). The lines with a ceiling function applied are those that would use only $0$-bit prefixing, whereas the continuous lines are those that would use an ideal scheme (even hex values are not truly continuous, but there are many more 'steps'). Illustrated is the proof of Sybil mitigation, as a result of proper proof-of-work scaling.

\par In a network with $100,000$ participants, the number malicious 'votes' that must be cast is $ > \frac{100000}{m}$. So, when $m = 0.5$ and $n = 50$, a normal user will have to calculate (about) $5.6\times{}10^8$ hashes to find a good nonce. Note that $n = 50$ is a more realistic baseline than $n = 32$. With a hash rate of $500KH/s$ and a wattage of of $650W$, reasonable empirical estimates of the hash rate and wattage of an average computer, it would take approximately $180$ seconds to find a nonce, or less than $1$\textcent. However, for an attack attempting to breach the consensus threshold $m$ of the network, it would take over $10,000$ hours or over $\$750$. Note that the price of electricity is estimated at $12\textcent$ per kilowatt-hour. 

\par One notable feature of our scheme is that the amount of power necessary to overcome the network is a constant. Because we are decreasing the amount of work required from each new vote based on the current amount, the requisite amount of power required to overcome the rest of the network is also decreased. We present this path as it is a relatively new one, and, for our planned purposes, it is likely more beneficial to lower the barrier to entry once the network becomes large. Discussed later \todo{put section here} are the ways we can combine our Sybil resistant protocols with others to chain together security. 
\par For networks that are to be used only in the short term, are large but run on devices without much computational power, or that would benefit more from a lower barrier to entry than would be allowed for with linear scaling of the hashing requirements, our scheme works best. Many autonomous networks fit this set of criteria, and for those that don't, a system where the required work is not dependant on the present work can be used. Hybrid systems, for example one in which the requisite work does not scale perfectly with the theoretical amount of work required for a Sybil attack overcoming $m$, are also very viable. There are really an endless number of possibilities, and our approach is presented here because no other scheme presents a working scaled difficulty model.


\subsection{Active Node Rearrangement}

\par We present here one possible methodology for a node rearrangement scheme. Approaches such as Self-Chord\todo{cite} have defined schemes similar to ours, with the notable absence of a method of fault tolerance in the face of adversaries. All other approaches are predicated on the idea that node rearrangement must be done by individual clients, using only the information at hand. These types of systems are inherently flawed, as local-optimum algorithms often fail in the face of higher level consensus attacks.

\par Our focus thus shifts to the problem of globalizing the knowledge scope of the network, such that individual nodes are not operating on disparate levels of information. A blockchain is used here, as it provides both a decentralized ledger-like data store, and an inherent higher-level consensus scheme (the hashing operations that extend the chain serve to 'validate' one branch, so nodes are assuredly operating on the same information)~\cite{Nakamoto:2008ti}. Our blockchain is modeled after that of Twister~\cite{Freitas:2013tb}; traditional cryptocurrency transactions are coupled with identity definition statements, and intermittent ($k$'th) blocks allow for the 'reporting' of all known proofs in a given nodes region (those proofs being the corresponding proof-of-work statements for stored data). Note that computational work as a measure of keyspace saturation is a better scheme than simple element reporting because it is inherently 'provable'; as discussed earlier, subverting this process can be accomplished only through the bulk creation of malicious proofs, which is computationally expensive.

\par The mechanics of the blockchain are well documented, so suffice to say that our variant leaves us with a working global knowledge scope; that is, all nodes in the network have access to metrics like keyspace density, and are thus able to independently (yet deterministically) coordinate a mass node rearrangement. 

\subsubsection{Local-Optimum Rearrangement}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{plots/histogram}
\caption{Keyspace Histogram}
\label{fig:Histo}
\end{figure}
\par An easy solution to node convergence is to have individual nodes distribute themselves normally (i.e. through consistent hashing), and then use some metric of local density to have them move slightly in a favorable direction. This is very similar to the reference local-optimum algorithms, but in our network there can be a bit more fine control. Illustrated in~\ref{fig:Histo} is a keyspace density histogram, fitted with splines of varying smoothness. It would be relatively trivial to have nodes perform some kind of binning operation, such as k-means clustering, to divide the keyspace into regions of distinct density. They would then just evaluate their position in the keyspace, and move slightly in the direction of increasing density.

\par TODO: TAKE THE DERIVATIVE OF THE SPLINE AT A POINT TO DETERMINE THE MOVEMENT OF THE NODE FROM ITS ORIGINAL KEYSPACE INDEX



\subsubsection{Global Rearrangement}
\todo{Todo: import $n'th$ arrangement section}

\par Testing of the arrangement scheme was done by the construction of a keyspace-density histrogram, similar to the 


\section{Conclusions}
\par A working decentralized, range-querayable data structure has wide-reaching implications for multiparty computation schemes. Things like decentralized sensornets are a very real possibility. Beyond the obvious applications for P2P networks, some of the novel ideas on identify verification and node distribution, for example the proposed scheme of deterministic $n'th$ arrangement, solve problems that plague all manner of decentralized systems. 



\printbibliography
\end{document}
